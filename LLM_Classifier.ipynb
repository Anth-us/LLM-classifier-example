{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwvjkoFqSwL/WMvo7MIKps",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anth-us/LLM-classifier-example/blob/main/LLM_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Classifier\n",
        "\n",
        "This is an example of using [function calling](https://anth.us/blog/how-ai-agents-do-things/) to transform unstructured input into structured data.  In this case, a binary classifier, mirroring the ML text classifiers that we [recently demonstrated](https://anth.us/blog/maximize-profit-not-intelligence/).\n",
        "\n",
        "In this example, we only send one request to the LLM instead of sending the second request with the results of the function.  In this use case the only thing that we want from the model is a response in a predictable, structured format.\n",
        "\n",
        "We do that by setting up one tool that accepts one required parameter: A boolean value: true or false.  This is the structured data that we want.\n",
        "\n",
        "We're using the LLM as a reasoning engine to decide that it should call the tool, since the prompt explicitly tells it to use the tool.  And to classify the sentence so that it can provide the required parameter."
      ],
      "metadata": {
        "id": "S9OZ0Ka52E0V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup\n",
        "\n",
        "import json\n",
        "import requests\n",
        "import openai\n",
        "from google.colab import userdata\n",
        "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
        "\n",
        "# Define the GPT model to use\n",
        "GPT_MODEL = \"gpt-3.5-turbo-1106\""
      ],
      "metadata": {
        "id": "JnBaj4nw3H6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDZRs2HFr6RR",
        "outputId": "98c8ed6b-d5a9-4385-c21d-de2eb6ea05af"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.5.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function for sending the request to OpenAI"
      ],
      "metadata": {
        "id": "wAbj3abgoeWh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "PeD09R4Hodhm"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "@retry(wait=wait_random_exponential(multiplier=1, max=40), stop=stop_after_attempt(3))\n",
        "def chat_completion_request(messages, tools=None, tool_choice=None, model=GPT_MODEL):\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Authorization\": \"Bearer \" + userdata.get('openai'),\n",
        "    }\n",
        "    json_data = {\"model\": model, \"messages\": messages}\n",
        "    if tools is not None:\n",
        "        json_data.update({\"tools\": tools})\n",
        "    if tool_choice is not None:\n",
        "        json_data.update({\"tool_choice\": tool_choice})\n",
        "    try:\n",
        "        response = requests.post(\n",
        "            \"https://api.openai.com/v1/chat/completions\",\n",
        "            headers=headers,\n",
        "            json=json_data,\n",
        "        )\n",
        "        return response\n",
        "    except Exception as e:\n",
        "        print(\"Unable to generate ChatCompletion response\")\n",
        "        print(f\"Exception: {e}\")\n",
        "        return e\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting the chat completion response"
      ],
      "metadata": {
        "id": "ZvYkKPpgrjXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the tool for yes/no classification\n",
        "tools = [\n",
        "  {\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "      \"name\": \"classify_yes_no\",\n",
        "      \"description\": \"Provide a clear 'yes' or 'no' as structured data.\",\n",
        "      \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "          \"classification\": {\n",
        "            \"type\": \"boolean\",\n",
        "            \"description\": \"Yes: TRUE, No: FALSE.\",\n",
        "          },\n",
        "        },\n",
        "        \"required\": [\"classification\"],\n",
        "      },\n",
        "    }\n",
        "  }\n",
        "]\n",
        "\n",
        "# Construct the message for the request\n",
        "message = {\n",
        "    \"role\": \"user\",\n",
        "    \"content\": \"\"\"Classify the following sentence as a 'yes' or 'no', based on\n",
        "      the criteria: \\\"Is this a question about Spanish immigration law?\\\"\n",
        "\n",
        "      Respond by calling the classify_yes_no function.\n",
        "\n",
        "      How do I find a lawyer to help me to move to Spain?\n",
        "    \"\"\"\n",
        "}\n",
        "\n",
        "# Make the request\n",
        "response = chat_completion_request(messages=[message], tools=tools)\n",
        "\n",
        "# Print the response\n",
        "print(json.dumps(response.json(), indent=2))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6L3FlMzuroPf",
        "outputId": "3c98f67c-4903-4f24-998f-17869f1fe78f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"id\": \"chatcmpl-8XFcoirpNKkY8cCFdUVZ278tBBpsy\",\n",
            "  \"object\": \"chat.completion\",\n",
            "  \"created\": 1702935578,\n",
            "  \"model\": \"gpt-3.5-turbo-1106\",\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"index\": 0,\n",
            "      \"message\": {\n",
            "        \"role\": \"assistant\",\n",
            "        \"content\": null,\n",
            "        \"tool_calls\": [\n",
            "          {\n",
            "            \"id\": \"call_BcAKr7rULC0TTZSz5LBgfH6h\",\n",
            "            \"type\": \"function\",\n",
            "            \"function\": {\n",
            "              \"name\": \"classify_yes_no\",\n",
            "              \"arguments\": \"{\\\"classification\\\":true}\"\n",
            "            }\n",
            "          }\n",
            "        ]\n",
            "      },\n",
            "      \"logprobs\": null,\n",
            "      \"finish_reason\": \"tool_calls\"\n",
            "    }\n",
            "  ],\n",
            "  \"usage\": {\n",
            "    \"prompt_tokens\": 120,\n",
            "    \"completion_tokens\": 16,\n",
            "    \"total_tokens\": 136\n",
            "  },\n",
            "  \"system_fingerprint\": \"fp_772e8125bb\"\n",
            "}\n"
          ]
        }
      ]
    }
  ]
}